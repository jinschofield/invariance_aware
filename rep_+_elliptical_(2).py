                       
"""Rep_+_Elliptical (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sWIZzmFYn_-x1IP78oNn-GL8Gy-STQr_
"""

import math, random, numpy as np
import torch, torch.nn as nn, torch.nn.functional as F, torch.optim as optim
import matplotlib.pyplot as plt
import seaborn as sns

SEED = 0
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)

torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

OFFLINE_NUM_ENVS = 256
OFFLINE_COLLECT_STEPS = 40000
OFFLINE_TRAIN_STEPS = 20000
OFFLINE_BATCH_SIZE = 256

PROBE_TRAIN_STEPS = 1500
PROBE_BATCH_SIZE = 512
PROBE_EVAL_SAMPLES = 5000
PROBE_TEST_FRAC = 0.2

MAZE_SIZE = 12
OBS_DIM = 5
N_ACTIONS = 4
MAX_EP_STEPS = 60

Z_DIM = 8
HIDDEN_DIM = 64

GEOM_P = 0.01
K_CAP = 10
CRTR_TEMP = float(np.sqrt(Z_DIM))
CRTR_REP = 2

PRINT_TRAIN_EVERY = 5000

_LAYOUT = torch.tensor([
    [1,1,1,1,1,1,1,1,1,1,1,1],
    [1,0,0,0,1,0,0,0,0,0,0,1],
    [1,1,1,0,1,0,1,1,1,1,0,1],
    [1,0,0,0,0,0,0,0,0,1,0,1],
    [1,0,1,1,1,1,1,1,0,1,0,1],
    [1,0,0,0,0,0,0,1,0,0,0,1],
    [1,1,1,1,0,1,0,1,1,1,0,1],
    [1,0,0,1,0,1,0,0,0,1,0,1],
    [1,0,1,1,0,1,1,1,0,1,0,1],
    [1,0,0,0,0,0,0,1,0,0,0,1],
    [1,1,1,1,1,1,0,1,1,1,0,1],
    [1,1,1,1,1,1,1,1,1,1,1,1],
], device=DEVICE, dtype=torch.bool)

_GOAL = torch.tensor([10, 10], device=DEVICE, dtype=torch.long)
_DELTAS = torch.tensor([[-1,0],[1,0],[0,-1],[0,1]], device=DEVICE, dtype=torch.long)

def _pos_norm_from_grid(pos_xy_long):
    return (pos_xy_long.float() / float(MAZE_SIZE - 1)) * 2.0 - 1.0

def _step_pos_with_layout(pos_xy, action, layout_bool):
    a = action.long().clamp(0, N_ACTIONS - 1)
    delta = _DELTAS[a]
    nxt = pos_xy + delta
    nxt[:, 0] = nxt[:, 0].clamp(0, MAZE_SIZE - 1)
    nxt[:, 1] = nxt[:, 1].clamp(0, MAZE_SIZE - 1)
    blocked = layout_bool[nxt[:, 0], nxt[:, 1]]
    return torch.where(blocked.unsqueeze(1), pos_xy, nxt)

class EpisodeIndex:
    def __init__(self, starts, lengths, num_envs):
        self.starts = starts
        self.lengths = lengths
        self.num_envs = int(num_envs)
        self.num_episodes = int(starts.numel())

@torch.no_grad()
def build_episode_index_strided(timestep, size, num_envs, device):
    starts, lengths = [], []
    nenv = int(num_envs)
    for e in range(nenv):
        ts = timestep[e:size:nenv]
        if ts.numel() == 0:
            continue
        start_pos = torch.nonzero(ts == 0).squeeze(-1)
        if start_pos.numel() == 0 or int(start_pos[0].item()) != 0:
            start_pos = torch.cat([torch.tensor([0], device=device, dtype=torch.long), start_pos], dim=0)
        for j in range(start_pos.numel()):
            sp = int(start_pos[j].item())
            ep_end = int(start_pos[j + 1].item()) if (j + 1) < start_pos.numel() else int(ts.shape[0])
            L = ep_end - sp
            if L >= 1:
                starts.append(e + sp * nenv)
                lengths.append(L)
    return EpisodeIndex(torch.tensor(starts, device=device, dtype=torch.long),
                        torch.tensor(lengths, device=device, dtype=torch.long),
                        nenv)

@torch.no_grad()
def sample_crtr_pairs_offline(buf, epi, batch_size, repetition_factor, k_cap, geom_p):
    base_n = batch_size // repetition_factor
    if base_n < 1:
        raise ValueError("batch_size must be >= repetition_factor")
    base_eps = torch.randint(0, epi.num_episodes, (base_n,), device=DEVICE)
    ep_ids = base_eps.repeat_interleave(repetition_factor)
    starts = epi.starts[ep_ids]
    lengths = epi.lengths[ep_ids]
    u = (torch.rand(batch_size, device=DEVICE) * lengths.float()).long()
    u = torch.minimum(u, lengths - 1)
    geom = torch.distributions.Geometric(probs=torch.tensor(float(geom_p), device=DEVICE))
    k = (geom.sample((batch_size,)).long() + 1).clamp(1, int(k_cap))
    uf = u + k
    idx_t = starts + u * epi.num_envs
    idx_f = starts + torch.minimum(uf, lengths - 1) * epi.num_envs
    idx_last = starts + (lengths - 1) * epi.num_envs
    s_t = buf.s[idx_t]
    s_f = buf.s[idx_f]
    overflow = uf >= lengths
    if overflow.any():
        s_f[overflow] = buf.sp[idx_last[overflow]]
    return s_t, s_f

class ReplayBufferPlus:
    def __init__(self, obs_dim, max_size, num_envs):
        self.obs_dim = int(obs_dim)
        self.max_size = int(max_size)
        self.num_envs = int(num_envs)

        self.s  = torch.empty((self.max_size, self.obs_dim), device=DEVICE)
        self.sp = torch.empty((self.max_size, self.obs_dim), device=DEVICE)
        self.a  = torch.empty((self.max_size,), device=DEVICE, dtype=torch.long)
        self.d  = torch.empty((self.max_size,), device=DEVICE, dtype=torch.bool)

        self.timestep = torch.empty((self.max_size,), device=DEVICE, dtype=torch.long)
        self.current_timestep = torch.zeros((self.num_envs,), device=DEVICE, dtype=torch.long)

        self.nuis = torch.empty((self.max_size,), device=DEVICE, dtype=torch.long)
        self.special = torch.empty((self.max_size,), device=DEVICE, dtype=torch.bool)

        self.size = 0

    def add_batch(self, s, a, sp, done, nuis=None, special=None):
        b = int(s.shape[0])
        end = self.size + b
        if end > self.max_size:
            raise RuntimeError("buffer overflow")
        idx = torch.arange(self.size, end, device=DEVICE)

        self.s[idx]  = s
        self.a[idx]  = a
        self.sp[idx] = sp
        self.d[idx]  = done

        self.timestep[idx] = self.current_timestep
        self.current_timestep = self.current_timestep + 1
        self.current_timestep[done] = 0

        self.nuis[idx] = 0 if nuis is None else nuis.long()
        self.special[idx] = False if special is None else special.bool()

        self.size = end

    def sample(self, batch_size):
        idx = torch.randint(0, self.size, (batch_size,), device=DEVICE)
        return self.s[idx], self.a[idx], self.sp[idx], self.d[idx]

class EncoderMLP(nn.Module):
    def __init__(self, obs_dim=OBS_DIM, z_dim=Z_DIM, hidden=HIDDEN_DIM):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(obs_dim, hidden), nn.ReLU(),
            nn.Linear(hidden, hidden), nn.ReLU(),
            nn.Linear(hidden, z_dim)
        )
    def forward(self, x):
        z = self.net(x)
        return F.normalize(z, dim=-1, eps=1e-8)

class InverseDynamicsLinear(nn.Module):
    def __init__(self, z_dim=Z_DIM, n_actions=N_ACTIONS):
        super().__init__()
        self.fc = nn.Linear(2 * z_dim, n_actions)
    def forward(self, z, zp):
        return self.fc(torch.cat([z, zp], dim=-1))

class ForwardDynamicsMLP(nn.Module):
    def __init__(self, z_dim=Z_DIM, hidden=HIDDEN_DIM, n_actions=N_ACTIONS):
        super().__init__()
        self.n_actions = int(n_actions)
        self.net = nn.Sequential(
            nn.Linear(z_dim + self.n_actions, hidden), nn.ReLU(),
            nn.Linear(hidden, hidden), nn.ReLU(),
            nn.Linear(hidden, z_dim)
        )
    def forward(self, z, a):
        a1 = F.one_hot(a, num_classes=self.n_actions).float()
        return self.net(torch.cat([z, a1], dim=-1))

class OfflineRepLearner(nn.Module):
    def __init__(self, method):
        super().__init__()
        self.method = method
        self.rep_enc = EncoderMLP().to(DEVICE)
        if method in ["IDM", "ICM"]:
            self.idm = InverseDynamicsLinear().to(DEVICE)
        if method == "ICM":
            self.fwd = ForwardDynamicsMLP().to(DEVICE)
        if method == "RND":
            self.rnd_target = EncoderMLP().to(DEVICE)
            for p in self.rnd_target.parameters():
                p.requires_grad = False
        params = list(self.rep_enc.parameters())
        if hasattr(self, "idm"):
            params += list(self.idm.parameters())
        if hasattr(self, "fwd"):
            params += list(self.fwd.parameters())
        self.opt = optim.Adam(params, lr=3e-4)

    def loss(self, buf, epi, batch_size):
        if self.method == "CRTR":
            rep = int(CRTR_REP)
            bs_eff = int(batch_size) - (int(batch_size) % rep)
            if bs_eff <= 0:
                bs_eff = rep
            s_t, s_f = sample_crtr_pairs_offline(buf, epi, bs_eff, rep, int(K_CAP), float(GEOM_P))
            z_t = self.rep_enc(s_t)
            z_f = self.rep_enc(s_f)
            logits = (z_t @ z_f.T) / float(CRTR_TEMP)
            return F.cross_entropy(logits, torch.arange(bs_eff, device=DEVICE))

        if self.method == "IDM":
            s, a, sp, _ = buf.sample(batch_size)
            z = self.rep_enc(s)
            zp = self.rep_enc(sp)
            return F.cross_entropy(self.idm(z, zp), a)

        if self.method == "ICM":
            s, a, sp, _ = buf.sample(batch_size)
            z = self.rep_enc(s)
            zp = self.rep_enc(sp)
            inv_loss = F.cross_entropy(self.idm(z, zp), a)
            pred_zp = self.fwd(z, a)
            fwd_loss = F.mse_loss(pred_zp, zp)
            return inv_loss + fwd_loss

        if self.method == "RND":
            s, _, _, _ = buf.sample(batch_size)
            z_pred = self.rep_enc(s)
            with torch.no_grad():
                z_targ = self.rnd_target(s)
            return F.mse_loss(z_pred, z_targ)

        raise ValueError(self.method)

    def train_steps(self, buf, epi, steps, batch_size, log_every=PRINT_TRAIN_EVERY):
        self.train()
        for t in range(int(steps)):
            l = self.loss(buf, epi, batch_size)
            self.opt.zero_grad(set_to_none=True)
            l.backward()
            self.opt.step()
            if log_every and ((t + 1) % int(log_every) == 0):
                print(f"    train step {t+1:>6}/{int(steps)} | loss={float(l.item()):.4f}", flush=True)

class BiscuitEncoder(nn.Module):
    def __init__(self, obs_dim=OBS_DIM, z_dim=Z_DIM, hidden=HIDDEN_DIM):
        super().__init__()
        self.trunk = nn.Sequential(
            nn.Linear(obs_dim, hidden), nn.SiLU(),
            nn.Linear(hidden, hidden), nn.SiLU(),
        )
        self.mu = nn.Linear(hidden, z_dim)
        self.logvar = nn.Linear(hidden, z_dim)
    def forward(self, x):
        h = self.trunk(x)
        return self.mu(h), self.logvar(h)

class BiscuitDecoder(nn.Module):
    def __init__(self, obs_dim=OBS_DIM, z_dim=Z_DIM, hidden=HIDDEN_DIM):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(z_dim, hidden), nn.SiLU(),
            nn.Linear(hidden, hidden), nn.SiLU(),
            nn.Linear(hidden, obs_dim),
        )
    def forward(self, z):
        return self.net(z)

class ActionEmbed(nn.Module):
    def __init__(self, n_actions=N_ACTIONS, a_dim=8):
        super().__init__()
        self.emb = nn.Embedding(n_actions, a_dim)
    def forward(self, a):
        return self.emb(a.long())

class BISCUIT_VAE(nn.Module):
    def __init__(self, obs_dim=OBS_DIM, z_dim=Z_DIM, n_actions=N_ACTIONS, a_dim=8, hidden=HIDDEN_DIM,
                 tau_start=1.0, tau_end=5.0, interaction_reg_weight=5e-4, beta_kl=1.0):
        super().__init__()
        self.obs_dim = obs_dim
        self.z_dim = z_dim
        self.beta_kl = float(beta_kl)

        self.enc = BiscuitEncoder(obs_dim, z_dim, hidden).to(DEVICE)
        self.dec = BiscuitDecoder(obs_dim, z_dim, hidden).to(DEVICE)
        self.aemb = ActionEmbed(n_actions, a_dim).to(DEVICE)

        self.mlp_I = nn.ModuleList([
            nn.Sequential(
                nn.Linear(z_dim + a_dim, hidden), nn.SiLU(),
                nn.Linear(hidden, hidden), nn.SiLU(),
                nn.Linear(hidden, 1)
            ) for _ in range(z_dim)
        ]).to(DEVICE)

        self.mlp_prior = nn.ModuleList([
            nn.Sequential(
                nn.Linear(z_dim + 1, hidden), nn.SiLU(),
                nn.Linear(hidden, hidden), nn.SiLU(),
                nn.Linear(hidden, 2)
            ) for _ in range(z_dim)
        ]).to(DEVICE)

        self.tau_start = float(tau_start)
        self.tau_end = float(tau_end)
        self.interaction_reg_weight = float(interaction_reg_weight)
        self.opt = optim.Adam(self.parameters(), lr=3e-4)

    @staticmethod
    def _reparam(mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def _tau(self, step, total_steps):
        if total_steps <= 1:
            return self.tau_end
        frac = float(step) / float(total_steps - 1)
        return self.tau_start + frac * (self.tau_end - self.tau_start)

    def _predict_Ihat(self, z_prev, a_emb, tau):
        inp = torch.cat([z_prev, a_emb], dim=-1)
        logits = torch.cat([mlp(inp) for mlp in self.mlp_I], dim=-1)
        I_hat = torch.tanh(logits * tau)
        return I_hat

    def _prior_params(self, z_prev, I_hat):
        mus, logvars = [], []
        for i in range(self.z_dim):
            out = self.mlp_prior[i](torch.cat([z_prev, I_hat[:, i:i+1]], dim=-1))
            mus.append(out[:, 0:1])
            logvars.append(out[:, 1:2])
        mu_p = torch.cat(mus, dim=-1)
        logvar_p = torch.cat(logvars, dim=-1).clamp(-10.0, 10.0)
        return mu_p, logvar_p

    @torch.no_grad()
    def encode_mean(self, x):
        self.eval()
        mu, _ = self.enc(x)
        return mu

    def train_step(self, x_prev, a, x_t, step_idx, total_steps):
        self.train()
        tau = self._tau(step_idx, total_steps)

        mu_prev, logvar_prev = self.enc(x_prev)
        z_prev = self._reparam(mu_prev, logvar_prev)

        mu_q, logvar_q = self.enc(x_t)
        z_t = self._reparam(mu_q, logvar_q)

        x_hat = self.dec(z_t)
        recon = F.mse_loss(x_hat, x_t, reduction="none").sum(dim=-1).mean()

        a_emb = self.aemb(a)
        I_hat = self._predict_Ihat(z_prev, a_emb, tau)
        mu_p, logvar_p = self._prior_params(z_prev, I_hat)

        var_q = torch.exp(logvar_q)
        var_p = torch.exp(logvar_p)
        kl = 0.5 * (logvar_p - logvar_q + (var_q + (mu_q - mu_p) ** 2) / var_p - 1.0).sum(dim=-1).mean()

        inter_reg = torch.clamp(I_hat + 1.0, min=0.0).pow(2).mean()
        loss = recon + self.beta_kl * kl + self.interaction_reg_weight * inter_reg

        self.opt.zero_grad(set_to_none=True)
        loss.backward()
        self.opt.step()
        return loss

class ScalarEnergy(nn.Module):
    def __init__(self, x_dim, emb=128, hidden=128):
        super().__init__()
        self.fx = nn.Sequential(
            nn.Linear(x_dim, hidden), nn.ReLU(),
            nn.Linear(hidden, hidden), nn.ReLU(),
            nn.Linear(hidden, emb)
        )
        self.hy = nn.Sequential(
            nn.Linear(1, hidden), nn.ReLU(),
            nn.Linear(hidden, hidden), nn.ReLU(),
            nn.Linear(hidden, emb)
        )
    def score(self, y, x):
        if y.ndim == 1:
            y = y.unsqueeze(-1)
        fx = self.fx(x)
        hy = self.hy(y)
        return (fx * hy).sum(dim=-1)

def sample_negatives_from_pool(pool, B, N):
    idx = torch.randint(0, pool.shape[0], (B, N), device=DEVICE)
    return pool[idx]

def info_nce_loss_scores(pos_score, neg_scores):
    denom = torch.exp(pos_score) + torch.exp(neg_scores).sum(dim=1)
    return -(pos_score - torch.log(denom + 1e-12)).mean()

def reg_terms_on_scores_and_grad(scores, y):
    grad = torch.autograd.grad(outputs=scores.sum(), inputs=y,
                               create_graph=True, retain_graph=True, allow_unused=False)[0]
    return (scores.pow(2).mean(), grad.pow(2).mean())

class CBM_Dynamics(nn.Module):
    def __init__(self, dS=OBS_DIM, n_actions=N_ACTIONS, emb=128, hidden=128):
        super().__init__()
        self.dS = int(dS)
        self.n_actions = int(n_actions)
        self.x_dim = self.dS + self.n_actions
        self.g = nn.ModuleList([ScalarEnergy(self.x_dim, emb=emb, hidden=hidden) for _ in range(self.dS)])
        self.psi = nn.ModuleList([ScalarEnergy(self.x_dim, emb=emb, hidden=hidden) for _ in range(self.dS)])
    def make_x(self, s, a):
        a1 = F.one_hot(a, num_classes=self.n_actions).float()
        return torch.cat([s, a1], dim=1)

class RewardPredictor(nn.Module):
    def __init__(self, x_dim, hidden=128):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(x_dim, hidden), nn.ReLU(),
            nn.Linear(hidden, hidden), nn.ReLU(),
            nn.Linear(hidden, 1)
        )
    def logp_bernoulli(self, x, r01):
        logits = self.net(x).squeeze(-1)
        return -F.binary_cross_entropy_with_logits(logits, r01.float(), reduction="none")

@torch.no_grad()
def build_value_pools(buf):
    pools = []
    sp = buf.sp[:buf.size]
    for i in range(OBS_DIM):
        pools.append(sp[:, i].clone().detach())
    return pools

def train_cbm_models(buf, steps=OFFLINE_TRAIN_STEPS, batch=OFFLINE_BATCH_SIZE,
                     N_NEG=64, eps_cmi=0.02, lam1=1e-6, lam2=1e-6, lr=3e-4):
    dyn = CBM_Dynamics(dS=OBS_DIM, n_actions=N_ACTIONS, emb=128, hidden=128).to(DEVICE)
    rew = RewardPredictor(x_dim=OBS_DIM + N_ACTIONS, hidden=128).to(DEVICE)
    opt_g = optim.Adam(dyn.g.parameters(), lr=lr)
    opt_psi = optim.Adam(dyn.psi.parameters(), lr=lr)
    opt_rew = optim.Adam(rew.parameters(), lr=lr)
    pools = build_value_pools(buf)

    def compute_reward(sp):
        xy = sp[:, :2]
        pos = torch.round(((xy + 1.0) * 0.5) * float(MAZE_SIZE - 1)).long().clamp(0, MAZE_SIZE - 1)
        reached = (pos == _GOAL.unsqueeze(0)).all(dim=1)
        return reached.float()

    print("    CBM: train g...", flush=True)
    for t in range(steps // 2):
        s, a, sp, _ = buf.sample(batch)
        x = dyn.make_x(s, a)
        for i in range(OBS_DIM):
            y_true = sp[:, i]
            y_neg = sample_negatives_from_pool(pools[i], batch, N_NEG)
            pos = dyn.g[i].score(y_true, x)
            neg = dyn.g[i].score(y_neg.reshape(-1), x.repeat_interleave(N_NEG, dim=0)).reshape(batch, N_NEG)
            loss_nce = info_nce_loss_scores(pos, neg)
            y_req = y_true.detach().clone().requires_grad_(True)
            pos_req = dyn.g[i].score(y_req, x)
            l2_s, l2_g = reg_terms_on_scores_and_grad(pos_req, y_req)
            loss = loss_nce + lam1 * l2_s + lam2 * l2_g
            opt_g.zero_grad(set_to_none=True)
            loss.backward()
            opt_g.step()
        if PRINT_TRAIN_EVERY and ((t + 1) % PRINT_TRAIN_EVERY == 0):
            print(f"      g step {t+1:>6}/{steps//2}", flush=True)

    print("    CBM: train psi...", flush=True)
    for t in range(steps // 2):
        s, a, sp, _ = buf.sample(batch)
        x_full = dyn.make_x(s, a)
        j = torch.randint(0, OBS_DIM, (1,), device=DEVICE).item()
        x_m = x_full.clone()
        x_m[:, j] = 0.0
        for i in range(OBS_DIM):
            y_true = sp[:, i]
            y_neg = sample_negatives_from_pool(pools[i], batch, N_NEG)
            pos = dyn.psi[i].score(y_true, x_m)
            neg = dyn.psi[i].score(y_neg.reshape(-1), x_m.repeat_interleave(N_NEG, dim=0)).reshape(batch, N_NEG)
            loss_nce = info_nce_loss_scores(pos, neg)
            y_req = y_true.detach().clone().requires_grad_(True)
            pos_req = dyn.psi[i].score(y_req, x_m)
            l2_s, l2_g = reg_terms_on_scores_and_grad(pos_req, y_req)
            loss = loss_nce + lam1 * l2_s + lam2 * l2_g
            opt_psi.zero_grad(set_to_none=True)
            loss.backward()
            opt_psi.step()
        if PRINT_TRAIN_EVERY and ((t + 1) % PRINT_TRAIN_EVERY == 0):
            print(f"      psi step {t+1:>6}/{steps//2}", flush=True)

    print("    CBM: train reward predictor...", flush=True)
    for t in range(steps // 2):
        s, a, sp, _ = buf.sample(batch)
        x_full = dyn.make_x(s, a)
        r = compute_reward(sp)
        if random.random() < 0.5:
            x_in = x_full
        else:
            j = torch.randint(0, OBS_DIM, (1,), device=DEVICE).item()
            x_in = x_full.clone()
            x_in[:, j] = 0.0
        logp = rew.logp_bernoulli(x_in, r)
        loss = -logp.mean()
        opt_rew.zero_grad(set_to_none=True)
        loss.backward()
        opt_rew.step()
        if PRINT_TRAIN_EVERY and ((t + 1) % PRINT_TRAIN_EVERY == 0):
            print(f"      rew step {t+1:>6}/{steps//2}", flush=True)

    print("    CBM: estimate CMI(dynamics) ...", flush=True)
    @torch.no_grad()
    def estimate_cmi_dyn(num_batches=30):
        cmi = torch.zeros((OBS_DIM + N_ACTIONS, OBS_DIM), device=DEVICE)
        counts = torch.zeros_like(cmi)
        for _ in range(num_batches):
            s, a, sp, _ = buf.sample(batch)
            x_full = dyn.make_x(s, a)
            for i in range(OBS_DIM):
                y_true = sp[:, i]
                y_neg = sample_negatives_from_pool(pools[i], batch, N_NEG)
                g_pos = dyn.g[i].score(y_true, x_full)
                g_neg = dyn.g[i].score(y_neg.reshape(-1), x_full.repeat_interleave(N_NEG, dim=0)).reshape(batch, N_NEG)
                for k in range(OBS_DIM + N_ACTIONS):
                    x_m = x_full.clone()
                    x_m[:, k] = 0.0
                    psi_pos = dyn.psi[i].score(y_true, x_m)
                    psi_neg = dyn.psi[i].score(y_neg.reshape(-1), x_m.repeat_interleave(N_NEG, dim=0)).reshape(batch, N_NEG)
                    w = F.softmax(psi_neg, dim=1)
                    phi_pos = g_pos - psi_pos
                    phi_neg = g_neg - psi_neg
                    num = (N_NEG + 1.0) * torch.exp(phi_pos)
                    den = torch.exp(phi_pos) + float(N_NEG) * (w * torch.exp(phi_neg)).sum(dim=1)
                    cmi_ik = torch.log((num / (den + 1e-12)) + 1e-12)
                    cmi[k, i] += cmi_ik.mean()
                    counts[k, i] += 1.0
        return cmi / (counts + 1e-12)

    print("    CBM: estimate CMI(reward) ...", flush=True)
    @torch.no_grad()
    def estimate_cmi_reward(num_batches=60):
        cmi_r = torch.zeros((OBS_DIM + N_ACTIONS,), device=DEVICE)
        counts = torch.zeros_like(cmi_r)
        for _ in range(num_batches):
            s, a, sp, _ = buf.sample(batch)
            x_full = dyn.make_x(s, a)
            r = compute_reward(sp)
            logp_full = rew.logp_bernoulli(x_full, r)
            for k in range(OBS_DIM + N_ACTIONS):
                x_m = x_full.clone()
                x_m[:, k] = 0.0
                logp_m = rew.logp_bernoulli(x_m, r)
                cmi_r[k] += (logp_full - logp_m).mean()
                counts[k] += 1.0
        return cmi_r / (counts + 1e-12)

    cmi_dyn = estimate_cmi_dyn(num_batches=30)
    cmi_rew = estimate_cmi_reward(num_batches=60)
    G_dyn = (cmi_dyn >= eps_cmi).bool()
    PR = (cmi_rew >= eps_cmi).bool()
    return dyn, rew, cmi_dyn, cmi_rew, G_dyn, PR

def ancestors_in_dyn_graph(G_dyn, reward_parent_mask):
    A = G_dyn[:OBS_DIM, :OBS_DIM]
    reward_state_parents = reward_parent_mask[:OBS_DIM].clone()
    parents_of = [torch.nonzero(A[:, i]).squeeze(-1).tolist() for i in range(OBS_DIM)]
    keep = set(torch.nonzero(reward_state_parents).squeeze(-1).tolist())
    queue = list(keep)
    while queue:
        v = queue.pop()
        for p in parents_of[v]:
            if p not in keep:
                keep.add(p)
                queue.append(p)
    keep_mask = torch.zeros(OBS_DIM, dtype=torch.bool, device=DEVICE)
    if len(keep) > 0:
        keep_mask[list(keep)] = True
    return keep_mask

_TETRA4 = (torch.tensor([
    [ 1.,  1.,  1.],
    [ 1., -1., -1.],
    [-1.,  1., -1.],
    [-1., -1.,  1.],
], device=DEVICE) / math.sqrt(3.0)).float()

def _one_hot3(k):
    return F.one_hot(k.long(), num_classes=3).float()

def _phase_sincos3(k, P):
    ang = (2.0 * math.pi / float(P)) * k.float()
    return torch.stack([torch.sin(ang), torch.cos(ang), torch.sin(2.0 * ang)], dim=-1)

class RotationMaze:
    N_CLASSES = 4
    def __init__(self, num_envs):
        self.num_envs = int(num_envs)
        self.layout = _LAYOUT
        self.pos = torch.zeros((self.num_envs, 2), device=DEVICE, dtype=torch.long)
        self.steps = torch.zeros((self.num_envs,), device=DEVICE, dtype=torch.long)
        self.free = torch.nonzero(~self.layout).long()
        self.rot = torch.zeros((self.num_envs,), device=DEVICE, dtype=torch.long)

    def reset(self, env_ids=None):
        if env_ids is None:
            env_ids = torch.arange(self.num_envs, device=DEVICE)
        idx = torch.randint(0, self.free.shape[0], (env_ids.numel(),), device=DEVICE)
        self.pos[env_ids] = self.free[idx]
        self.steps[env_ids] = 0
        self.rot[env_ids] = torch.randint(0, 4, (env_ids.numel(),), device=DEVICE)
        return self._get_obs(env_ids)

    def _get_obs(self, env_ids=None):
        if env_ids is None:
            env_ids = torch.arange(self.num_envs, device=DEVICE)
        xy = _pos_norm_from_grid(self.pos[env_ids])
        rot_feat = _TETRA4[self.rot[env_ids]]
        return torch.cat([xy, rot_feat], dim=1)

    def step(self, a):
        self.pos = _step_pos_with_layout(self.pos, a, self.layout)
        self.steps += 1
        reached = (self.pos == _GOAL.unsqueeze(0)).all(dim=1)
        timeouts = self.steps >= MAX_EP_STEPS
        done = reached | timeouts
        next_obs = self._get_obs()
        reset_obs = next_obs.clone()
        if done.any():
            ids = torch.nonzero(done).squeeze(-1)
            self.reset(ids)
            reset_obs[ids] = self._get_obs(ids)
        return next_obs, done, reset_obs

    @torch.no_grad()
    def sample_invariance_pairs(self, num_samples=2048):
        idx = torch.randint(0, self.free.shape[0], (num_samples,), device=DEVICE)
        pos = self.free[idx]
        r1 = torch.randint(0, 4, (num_samples,), device=DEVICE)
        r2 = torch.randint(0, 4, (num_samples,), device=DEVICE)
        xy = _pos_norm_from_grid(pos)
        o1 = torch.cat([xy, _TETRA4[r1]], dim=1)
        o2 = torch.cat([xy, _TETRA4[r2]], dim=1)
        return o1, o2

class PeriodicMaze:
    P = 8
    N_CLASSES = P
    def __init__(self, num_envs):
        self.num_envs = int(num_envs)
        self.layout = _LAYOUT
        self.pos = torch.zeros((self.num_envs, 2), device=DEVICE, dtype=torch.long)
        self.steps = torch.zeros((self.num_envs,), device=DEVICE, dtype=torch.long)
        self.free = torch.nonzero(~self.layout).long()
        self.phase = torch.zeros((self.num_envs,), device=DEVICE, dtype=torch.long)

    def reset(self, env_ids=None):
        if env_ids is None:
            env_ids = torch.arange(self.num_envs, device=DEVICE)
        idx = torch.randint(0, self.free.shape[0], (env_ids.numel(),), device=DEVICE)
        self.pos[env_ids] = self.free[idx]
        self.steps[env_ids] = 0
        self.phase[env_ids] = torch.randint(0, self.P, (env_ids.numel(),), device=DEVICE)
        return self._get_obs(env_ids)

    def _get_obs(self, env_ids=None):
        if env_ids is None:
            env_ids = torch.arange(self.num_envs, device=DEVICE)
        xy = _pos_norm_from_grid(self.pos[env_ids])
        ph = _phase_sincos3(self.phase[env_ids], self.P)
        return torch.cat([xy, ph], dim=1)

    def step(self, a):
        self.pos = _step_pos_with_layout(self.pos, a, self.layout)
        self.phase = (self.phase + 1) % self.P
        self.steps += 1
        reached = (self.pos == _GOAL.unsqueeze(0)).all(dim=1)
        timeouts = self.steps >= MAX_EP_STEPS
        done = reached | timeouts
        next_obs = self._get_obs()
        reset_obs = next_obs.clone()
        if done.any():
            ids = torch.nonzero(done).squeeze(-1)
            self.reset(ids)
            reset_obs[ids] = self._get_obs(ids)
        return next_obs, done, reset_obs

    @torch.no_grad()
    def sample_invariance_pairs(self, num_samples=2048):
        idx = torch.randint(0, self.free.shape[0], (num_samples,), device=DEVICE)
        pos = self.free[idx]
        p1 = torch.randint(0, self.P, (num_samples,), device=DEVICE)
        p2 = torch.randint(0, self.P, (num_samples,), device=DEVICE)
        xy = _pos_norm_from_grid(pos)
        o1 = torch.cat([xy, _phase_sincos3(p1, self.P)], dim=1)
        o2 = torch.cat([xy, _phase_sincos3(p2, self.P)], dim=1)
        return o1, o2

class SlipperyDelayMaze:
    D = 3
    N_CLASSES = D
    def __init__(self, num_envs):
        self.num_envs = int(num_envs)
        self.layout = _LAYOUT
        self.pos = torch.zeros((self.num_envs, 2), device=DEVICE, dtype=torch.long)
        self.steps = torch.zeros((self.num_envs,), device=DEVICE, dtype=torch.long)
        self.free = torch.nonzero(~self.layout).long()
        self.qptr = torch.zeros((self.num_envs,), device=DEVICE, dtype=torch.long)
        self.queue = torch.randint(0, N_ACTIONS, (self.num_envs, self.D), device=DEVICE, dtype=torch.long)

    def reset(self, env_ids=None):
        if env_ids is None:
            env_ids = torch.arange(self.num_envs, device=DEVICE)
        idx = torch.randint(0, self.free.shape[0], (env_ids.numel(),), device=DEVICE)
        self.pos[env_ids] = self.free[idx]
        self.steps[env_ids] = 0
        self.qptr[env_ids] = torch.randint(0, self.D, (env_ids.numel(),), device=DEVICE)
        self.queue[env_ids] = torch.randint(0, N_ACTIONS, (env_ids.numel(), self.D), device=DEVICE)
        return self._get_obs(env_ids)

    def _get_obs(self, env_ids=None):
        if env_ids is None:
            env_ids = torch.arange(self.num_envs, device=DEVICE)
        xy = _pos_norm_from_grid(self.pos[env_ids])
        ph = _one_hot3(self.qptr[env_ids])
        return torch.cat([xy, ph], dim=1)

    def step(self, a_cmd):
        a_exec = self.queue[torch.arange(self.num_envs, device=DEVICE), self.qptr]
        self.queue[torch.arange(self.num_envs, device=DEVICE), self.qptr] = a_cmd.long().clamp(0, N_ACTIONS-1)
        self.qptr = (self.qptr + 1) % self.D

        self.pos = _step_pos_with_layout(self.pos, a_exec, self.layout)
        self.steps += 1
        reached = (self.pos == _GOAL.unsqueeze(0)).all(dim=1)
        timeouts = self.steps >= MAX_EP_STEPS
        done = reached | timeouts
        next_obs = self._get_obs()
        reset_obs = next_obs.clone()
        if done.any():
            ids = torch.nonzero(done).squeeze(-1)
            self.reset(ids)
            reset_obs[ids] = self._get_obs(ids)
        return next_obs, done, reset_obs

    @torch.no_grad()
    def sample_invariance_pairs(self, num_samples=2048):
        idx = torch.randint(0, self.free.shape[0], (num_samples,), device=DEVICE)
        pos = self.free[idx]
        p1 = torch.randint(0, self.D, (num_samples,), device=DEVICE)
        p2 = torch.randint(0, self.D, (num_samples,), device=DEVICE)
        xy = _pos_norm_from_grid(pos)
        o1 = torch.cat([xy, _one_hot3(p1)], dim=1)
        o2 = torch.cat([xy, _one_hot3(p2)], dim=1)
        return o1, o2


def _make_open_plate_layout():
    L = torch.ones((MAZE_SIZE, MAZE_SIZE), device=DEVICE, dtype=torch.bool)
    L[1:-1, 1:-1] = False
    return L

_LAYOUT_PLATE = _make_open_plate_layout()
_SPIN_DELTAS = torch.tensor([[0,1],[1,0],[0,-1],[-1,0]], device=DEVICE, dtype=torch.long)

class TeacupMaze:
    P = 4
    N_CLASSES = P
    def __init__(self, num_envs):
        self.num_envs = int(num_envs)
        self.layout = _LAYOUT_PLATE
        self.pos = torch.zeros((self.num_envs, 2), device=DEVICE, dtype=torch.long)
        self.steps = torch.zeros((self.num_envs,), device=DEVICE, dtype=torch.long)
        self.free = torch.nonzero(~self.layout).long()

        self.centers = torch.tensor([[3,3],[3,8],[8,3],[8,8],[6,6]], device=DEVICE, dtype=torch.long)
        self.rad2 = 2
        self.phase = torch.zeros((self.num_envs,), device=DEVICE, dtype=torch.long)
        self.cup_cells = self._compute_cup_cells()

    def _compute_cup_cells(self):
        free = self.free
        d2 = ((free[:, None, :] - self.centers[None, :, :]).float().pow(2)).sum(dim=-1)
        in_cup = (d2.min(dim=1).values <= float(self.rad2))
        return free[in_cup]

    def _which_cup(self, pos_xy):
        d2 = ((pos_xy[:, None, :] - self.centers[None, :, :]).float().pow(2)).sum(dim=-1)
        min_d2, cid = d2.min(dim=1)
        in_cup = min_d2 <= float(self.rad2)
        return in_cup, cid

    def reset(self, env_ids=None):
        if env_ids is None:
            env_ids = torch.arange(self.num_envs, device=DEVICE)
        idx = torch.randint(0, self.free.shape[0], (env_ids.numel(),), device=DEVICE)
        self.pos[env_ids] = self.free[idx]
        self.steps[env_ids] = 0
        self.phase[env_ids] = torch.randint(0, self.P, (env_ids.numel(),), device=DEVICE)
        return self._get_obs(env_ids)

    def _get_obs(self, env_ids=None):
        if env_ids is None:
            env_ids = torch.arange(self.num_envs, device=DEVICE)
        pos = self.pos[env_ids]
        xy = _pos_norm_from_grid(pos)
        in_cup, cid = self._which_cup(pos)
        local_phase = (self.phase[env_ids] + cid) % self.P
        feat = _TETRA4[local_phase] * in_cup.float().unsqueeze(1)
        return torch.cat([xy, feat], dim=1)

    def step(self, a):
        pos1 = _step_pos_with_layout(self.pos, a, self.layout)
        in_cup, cid = self._which_cup(pos1)
        local_phase = (self.phase + cid) % self.P
        drift = _SPIN_DELTAS[local_phase] * in_cup.long().unsqueeze(1)
        pos2 = pos1 + drift
        pos2[:, 0] = pos2[:, 0].clamp(0, MAZE_SIZE - 1)
        pos2[:, 1] = pos2[:, 1].clamp(0, MAZE_SIZE - 1)
        blocked = self.layout[pos2[:, 0], pos2[:, 1]]
        self.pos = torch.where(blocked.unsqueeze(1), pos1, pos2)

        self.phase = (self.phase + 1) % self.P
        self.steps += 1
        reached = (self.pos == _GOAL.unsqueeze(0)).all(dim=1)
        timeouts = self.steps >= MAX_EP_STEPS
        done = reached | timeouts
        next_obs = self._get_obs()
        reset_obs = next_obs.clone()
        if done.any():
            ids = torch.nonzero(done).squeeze(-1)
            self.reset(ids)
            reset_obs[ids] = self._get_obs(ids)
        return next_obs, done, reset_obs

    @torch.no_grad()
    def current_special(self):
        in_cup, _ = self._which_cup(self.pos)
        return in_cup

    @torch.no_grad()
    def current_nuis(self):
        in_cup, cid = self._which_cup(self.pos)
        return (self.phase + cid) % self.P

    @torch.no_grad()
    def sample_invariance_pairs(self, num_samples=2048):
        idx = torch.randint(0, self.cup_cells.shape[0], (num_samples,), device=DEVICE)
        pos = self.cup_cells[idx]
        p1 = torch.randint(0, self.P, (num_samples,), device=DEVICE)
        p2 = torch.randint(0, self.P, (num_samples,), device=DEVICE)
        xy = _pos_norm_from_grid(pos)
        o1 = torch.cat([xy, _TETRA4[p1]], dim=1)
        o2 = torch.cat([xy, _TETRA4[p2]], dim=1)
        return o1, o2


@torch.no_grad()
def collect_offline_dataset(env_ctor, total_transitions, num_envs):
    iters = int(math.ceil(float(total_transitions) / float(num_envs)))
    buf = ReplayBufferPlus(OBS_DIM, iters * num_envs + 2048, num_envs)
    env = env_ctor(num_envs)
    obs = env.reset()
    for _ in range(iters):
        a = torch.randint(0, N_ACTIONS, (num_envs,), device=DEVICE)

        if isinstance(env, TeacupMaze):
            nuis = env.current_nuis()
            special = env.current_special()
        else:
            nuis = getattr(env, "rot", getattr(env, "phase", getattr(env, "qptr", None)))
            if nuis is None:
                nuis = torch.zeros((num_envs,), device=DEVICE, dtype=torch.long)
            special = torch.zeros((num_envs,), device=DEVICE, dtype=torch.bool)

        sp, done, reset_obs = env.step(a)
        buf.add_batch(obs, a, sp, done, nuis=nuis, special=special)
        obs = reset_obs
    return buf, env

def _encode_batch(encode_fn, x):
    z = encode_fn(x)
    if z.ndim != 2:
        z = z.reshape(z.shape[0], -1)
    return z

@torch.no_grad()
def _make_split_indices(N, test_frac=PROBE_TEST_FRAC, seed=SEED):
    if N <= 1:
        return torch.arange(N, device=DEVICE), torch.arange(0, device=DEVICE)
    g = torch.Generator(device=DEVICE)
    g.manual_seed(int(seed))
    perm = torch.randperm(N, generator=g, device=DEVICE)
    n_test = int(max(1, round(float(N) * float(test_frac))))
    n_test = min(n_test, N - 1)
    test_idx = perm[:n_test]
    train_idx = perm[n_test:]
    return train_idx, test_idx

@torch.no_grad()
def _label_stats(y, num_classes):
    counts = torch.bincount(y.long(), minlength=int(num_classes)).float()
    probs = (counts / counts.sum().clamp_min(1.0)).detach().cpu().numpy()
    return probs

def run_linear_probe_any(encode_fn, obs, y, num_classes, split_seed=SEED):
    N = int(obs.shape[0])
    if N < 2:
        return float("nan"), float("nan")

    train_idx, test_idx = _make_split_indices(N, PROBE_TEST_FRAC, seed=split_seed)

    with torch.no_grad():
        z0 = _encode_batch(encode_fn, obs[train_idx[:min(64, train_idx.numel())]])
        rep_dim = int(z0.shape[1])

    probe = nn.Linear(rep_dim, int(num_classes)).to(DEVICE)
    opt = optim.Adam(probe.parameters(), lr=1e-3)


    for _ in range(PROBE_TRAIN_STEPS):
        idx = train_idx[torch.randint(0, train_idx.numel(), (min(PROBE_BATCH_SIZE, train_idx.numel()),), device=DEVICE)]
        with torch.no_grad():
            z = _encode_batch(encode_fn, obs[idx])
        logits = probe(z)
        loss = F.cross_entropy(logits, y[idx])
        opt.zero_grad(set_to_none=True)
        loss.backward()
        opt.step()


    m = min(PROBE_EVAL_SAMPLES, int(test_idx.numel()))
    idx = test_idx[torch.randint(0, test_idx.numel(), (m,), device=DEVICE)]
    with torch.no_grad():
        z = _encode_batch(encode_fn, obs[idx])
        logits = probe(z)
        yb = y[idx]
        acc = (logits.argmax(1) == yb).float().mean().item()
        ce = F.cross_entropy(logits, yb).item()
        mi = max(0.0, float(np.log(float(num_classes)) - ce))
    return acc, mi

def run_xy_regression_probe(encode_fn, obs, split_seed=SEED):
    N = int(obs.shape[0])
    if N < 2:
        return float("nan")

    train_idx, test_idx = _make_split_indices(N, PROBE_TEST_FRAC, seed=split_seed)

    with torch.no_grad():
        z0 = _encode_batch(encode_fn, obs[train_idx[:min(64, train_idx.numel())]])
        rep_dim = int(z0.shape[1])

    head = nn.Linear(rep_dim, 2).to(DEVICE)
    opt = optim.Adam(head.parameters(), lr=1e-3)
    target = obs[:, :2]


    for _ in range(PROBE_TRAIN_STEPS):
        idx = train_idx[torch.randint(0, train_idx.numel(), (min(PROBE_BATCH_SIZE, train_idx.numel()),), device=DEVICE)]
        with torch.no_grad():
            z = _encode_batch(encode_fn, obs[idx])
        pred = head(z)
        loss = F.mse_loss(pred, target[idx])
        opt.zero_grad(set_to_none=True)
        loss.backward()
        opt.step()


    m = min(PROBE_EVAL_SAMPLES, int(test_idx.numel()))
    idx = test_idx[torch.randint(0, test_idx.numel(), (m,), device=DEVICE)]
    with torch.no_grad():
        z = _encode_batch(encode_fn, obs[idx])
        pred = head(z)
        mse = F.mse_loss(pred, target[idx]).item()
    return mse

@torch.no_grad()
def invariance_metric_from_pairs(encode_fn, obs1, obs2):
    z1 = _encode_batch(encode_fn, obs1)
    z2 = _encode_batch(encode_fn, obs2)
    return (z1 - z2).pow(2).sum(dim=1).mean().item()

def action_probe_from_pairs(encode_fn, s1, s2, a, n_actions=N_ACTIONS, split_seed=SEED):
    N = int(s1.shape[0])
    if N < 2:
        return float("nan")

    train_idx, test_idx = _make_split_indices(N, PROBE_TEST_FRAC, seed=split_seed)

    with torch.no_grad():
        z1 = _encode_batch(encode_fn, s1[train_idx[:min(64, train_idx.numel())]])
        rep_dim = int(z1.shape[1])

    clf = nn.Linear(2 * rep_dim, int(n_actions)).to(DEVICE)
    opt = optim.Adam(clf.parameters(), lr=1e-3)


    for _ in range(PROBE_TRAIN_STEPS):
        idx = train_idx[torch.randint(0, train_idx.numel(), (min(PROBE_BATCH_SIZE, train_idx.numel()),), device=DEVICE)]
        with torch.no_grad():
            zt = _encode_batch(encode_fn, s1[idx])
            zu = _encode_batch(encode_fn, s2[idx])
        logits = clf(torch.cat([zt, zu], dim=1))
        loss = F.cross_entropy(logits, a[idx])
        opt.zero_grad(set_to_none=True)
        loss.backward()
        opt.step()


    m = min(PROBE_EVAL_SAMPLES, int(test_idx.numel()))
    idx = test_idx[torch.randint(0, test_idx.numel(), (m,), device=DEVICE)]
    with torch.no_grad():
        zt = _encode_batch(encode_fn, s1[idx])
        zu = _encode_batch(encode_fn, s2[idx])
        logits = clf(torch.cat([zt, zu], dim=1))
        acc = (logits.argmax(1) == a[idx]).float().mean().item()
    return acc

@torch.no_grad()
def sample_delayed_pairs_for_slippery(buf, delay_steps, num_envs, max_pairs=20000):
    nenv = int(num_envs)
    D = int(delay_steps)
    max_base = buf.size - (D * nenv)
    base = torch.arange(0, max_base, device=DEVICE)
    idxD = base + D * nenv
    good = (buf.timestep[idxD] == buf.timestep[base] + D)
    base = base[good]
    if base.numel() == 0:
        return None
    if base.numel() > max_pairs:
        sel = torch.randint(0, base.numel(), (max_pairs,), device=DEVICE)
        base = base[sel]
    idxD = base + D * nenv

    s_delay = buf.s[idxD]
    sp_delay = buf.sp[idxD]
    a_label = buf.a[base]
    return s_delay, sp_delay, a_label

def _tight_ylim(ax, vals, pad_frac=0.15):
    vmin = float(np.min(vals))
    vmax = float(np.max(vals))
    if np.isclose(vmin, vmax):
        pad = 1e-3 if vmax == 0 else abs(vmax) * 0.05
    else:
        pad = (vmax - vmin) * pad_frac
    ax.set_ylim(vmin - pad, vmax + pad)

def plot_env_bars(env_name, rows, metric_key, title, chance_line=None):
    names = [r["method"] for r in rows]
    vals = np.array([r[metric_key] for r in rows], dtype=np.float64)
    plt.figure(figsize=(22, 4))
    sns.barplot(x=names, y=vals)
    if chance_line is not None:
        plt.axhline(chance_line, ls="--")
    plt.title(f"{env_name}: {title}")
    ax = plt.gca()
    _tight_ylim(ax, vals)
    plt.xticks(rotation=45, ha="right")
    plt.tight_layout()
    plt.show()

def run_master_sweep_4envs(
    CRTR_R_LIST=(1, 2, 4, 8, 16, 32, 64),
    inv_samples=2048,
):
    env_specs = [
        {"name": "Rotation",    "ctor": RotationMaze,      "classes": RotationMaze.N_CLASSES,      "probe_mask": None},
        {"name": "Periodicity", "ctor": PeriodicMaze,      "classes": PeriodicMaze.N_CLASSES,      "probe_mask": None},
        {"name": "Slippery",    "ctor": SlipperyDelayMaze, "classes": SlipperyDelayMaze.N_CLASSES, "probe_mask": None},
        {"name": "Teacups",     "ctor": TeacupMaze,        "classes": TeacupMaze.N_CLASSES,        "probe_mask": "special_only"},
    ]

    all_results = []

    for spec in env_specs:
        print("\n" + "=" * 92)
        print(f"ENV = {spec['name']}")
        print("=" * 92)

        print("Collecting offline dataset...", flush=True)
        buf, env = collect_offline_dataset(spec["ctor"], OFFLINE_COLLECT_STEPS, OFFLINE_NUM_ENVS)
        epi = build_episode_index_strided(buf.timestep, buf.size, OFFLINE_NUM_ENVS, DEVICE)

        obs_all = buf.s[:buf.size]
        y_all = buf.nuis[:buf.size].long()

        if spec["probe_mask"] == "special_only":
            mask = buf.special[:buf.size]
            obs = obs_all[mask]
            y = y_all[mask]
            print(f"  Teacups special fraction = {float(mask.float().mean().item()):.3f} ({int(mask.sum())}/{int(mask.numel())})", flush=True)
            if obs.shape[0] < 1000:
                print(f"  WARNING: few special samples: {obs.shape[0]}.", flush=True)
            if obs.shape[0] < 2:
                raise RuntimeError("Teacups produced too few special samples to run probes.")
        else:
            obs = obs_all
            y = y_all

        num_classes = int(spec["classes"])
        chance = 1.0 / float(num_classes)

        probs = _label_stats(y, num_classes)
        print(f"  nuisance label probs (len={num_classes}): {np.round(probs, 3)}", flush=True)

        obs1, obs2 = env.sample_invariance_pairs(inv_samples)
        with torch.no_grad():
            same_xy = (obs1[:, :2] - obs2[:, :2]).abs().max().item()
        if same_xy > 1e-6:
            print(f"  WARNING: invariance pairs XY mismatch max={same_xy:.2e}", flush=True)

        env_rows = []

        def log_row(method_name, nuisance_acc, nuisance_mi, inv, xy_mse, extra=None):
            row = {
                "env": spec["name"],
                "method": method_name,
                "nuis_acc": float(nuisance_acc),
                "nuis_mi": float(nuisance_mi),
                "inv": float(inv),
                "xy_mse": float(xy_mse),
            }
            if extra is not None:
                row.update(extra)
            all_results.append(row)
            env_rows.append(row)

        global CRTR_REP
        orig_rep = CRTR_REP
        for r in CRTR_R_LIST:
            CRTR_REP = int(r)
            name = f"CRTR_R{r}"
            print(f"\n>>> {name}", flush=True)
            learner = OfflineRepLearner("CRTR").to(DEVICE)

            print("  train:", flush=True)
            learner.train_steps(buf, epi, OFFLINE_TRAIN_STEPS, OFFLINE_BATCH_SIZE, log_every=PRINT_TRAIN_EVERY)

            enc = lambda x: learner.rep_enc(x)

            print("  probe/eval:", flush=True)
            acc, mi = run_linear_probe_any(enc, obs, y, num_classes, split_seed=SEED)
            inv = invariance_metric_from_pairs(enc, obs1, obs2)
            xy_mse = run_xy_regression_probe(enc, obs_all, split_seed=SEED)

            print(f"  done | nuis_acc={acc:.3f} (chance={chance:.3f}) | MI={mi:.3f} | inv={inv:.4f} | xy_mse={xy_mse:.4f}", flush=True)
            log_row(name, acc, mi, inv, xy_mse)

        CRTR_REP = orig_rep

        for method in ["IDM", "ICM", "RND"]:
            print(f"\n>>> {method}", flush=True)
            learner = OfflineRepLearner(method).to(DEVICE)

            print("  train:", flush=True)
            learner.train_steps(buf, epi, OFFLINE_TRAIN_STEPS, OFFLINE_BATCH_SIZE, log_every=PRINT_TRAIN_EVERY)

            enc = lambda x: learner.rep_enc(x)

            print("  probe/eval:", flush=True)
            acc, mi = run_linear_probe_any(enc, obs, y, num_classes, split_seed=SEED)
            inv = invariance_metric_from_pairs(enc, obs1, obs2)
            xy_mse = run_xy_regression_probe(enc, obs_all, split_seed=SEED)
            print(f"  done | nuis_acc={acc:.3f} (chance={chance:.3f}) | MI={mi:.3f} | inv={inv:.4f} | xy_mse={xy_mse:.4f}", flush=True)

            extra = {}
            if spec["name"] == "Slippery":
                pairs = sample_delayed_pairs_for_slippery(buf, delay_steps=SlipperyDelayMaze.D, num_envs=OFFLINE_NUM_ENVS)
                if pairs is not None:
                    sD, spD, a0 = pairs
                    delay_acc = action_probe_from_pairs(enc, sD, spD, a0, split_seed=SEED)
                    extra["delay_ID_acc"] = float(delay_acc)
                    print(f"       (slippery extra) delay_ID_acc={delay_acc:.3f}", flush=True)
                else:
                    extra["delay_ID_acc"] = float("nan")

            log_row(method, acc, mi, inv, xy_mse, extra=extra)

        print(f"\n>>> BISCUIT", flush=True)
        model = BISCUIT_VAE().to(DEVICE)
        for t in range(OFFLINE_TRAIN_STEPS):
            s, a, sp, done = buf.sample(OFFLINE_BATCH_SIZE)
            loss = model.train_step(s, a, sp, t, OFFLINE_TRAIN_STEPS)
            if PRINT_TRAIN_EVERY and ((t + 1) % PRINT_TRAIN_EVERY == 0):
                print(f"    BISCUIT step {t+1:>6}/{OFFLINE_TRAIN_STEPS} | loss={float(loss.item()):.4f}", flush=True)

        enc = lambda x: model.encode_mean(x)
        acc, mi = run_linear_probe_any(enc, obs, y, num_classes, split_seed=SEED)
        inv = invariance_metric_from_pairs(enc, obs1, obs2)
        xy_mse = run_xy_regression_probe(enc, obs_all, split_seed=SEED)

        print(f"  done | nuis_acc={acc:.3f} (chance={chance:.3f}) | MI={mi:.3f} | inv={inv:.4f} | xy_mse={xy_mse:.4f}", flush=True)

        extra = {}
        if spec["name"] == "Slippery":
            pairs = sample_delayed_pairs_for_slippery(buf, delay_steps=SlipperyDelayMaze.D, num_envs=OFFLINE_NUM_ENVS)
            if pairs is not None:
                sD, spD, a0 = pairs
                delay_acc = action_probe_from_pairs(enc, sD, spD, a0, split_seed=SEED)
                extra["delay_ID_acc"] = float(delay_acc)
                print(f"       (slippery extra) delay_ID_acc={delay_acc:.3f}", flush=True)
            else:
                extra["delay_ID_acc"] = float("nan")

        log_row("BISCUIT", acc, mi, inv, xy_mse, extra=extra)

        print(f"\n>>> CBM", flush=True)
        dyn, rew, cmi_dyn, cmi_rew, G_dyn, PR = train_cbm_models(buf)
        keep_state = ancestors_in_dyn_graph(G_dyn, PR)
        enc = lambda x: x[:, keep_state]

        acc, mi = run_linear_probe_any(enc, obs, y, num_classes, split_seed=SEED)
        inv = invariance_metric_from_pairs(enc, obs1, obs2)
        xy_mse = run_xy_regression_probe(enc, obs_all, split_seed=SEED)

        print(f"  done | nuis_acc={acc:.3f} (chance={chance:.3f}) | MI={mi:.3f} | inv={inv:.4f} | xy_mse={xy_mse:.4f} | abstraction={int(keep_state.sum().item())}/{OBS_DIM}", flush=True)

        extra = {"abstraction_size": int(keep_state.sum().item())}
        if spec["name"] == "Slippery":
            pairs = sample_delayed_pairs_for_slippery(buf, delay_steps=SlipperyDelayMaze.D, num_envs=OFFLINE_NUM_ENVS)
            if pairs is not None:
                sD, spD, a0 = pairs
                delay_acc = action_probe_from_pairs(enc, sD, spD, a0, split_seed=SEED)
                extra["delay_ID_acc"] = float(delay_acc)
                print(f"       (slippery extra) delay_ID_acc={delay_acc:.3f}", flush=True)
            else:
                extra["delay_ID_acc"] = float("nan")

        log_row("CBM", acc, mi, inv, xy_mse, extra=extra)

        plot_env_bars(spec["name"], env_rows, "nuis_acc", "Nuisance probe acc (low=better)", chance_line=chance)
        plot_env_bars(spec["name"], env_rows, "nuis_mi",  "MI proxy (low=better)")
        plot_env_bars(spec["name"], env_rows, "inv",      "Geometric invariance (low=better)")
        plot_env_bars(spec["name"], env_rows, "xy_mse",   "XY regression MSE (low=better)")

        if spec["name"] == "Slippery":
            rows2 = [r for r in env_rows if "delay_ID_acc" in r]
            if len(rows2) > 0:
                plot_env_bars(spec["name"], rows2, "delay_ID_acc", "Delayed inverse dynamics probe acc (high=better)")


    print("\n" + "=" * 92)
    print("SUMMARY (last 20 rows):")
    print("=" * 92)
    for r in all_results[-min(20, len(all_results)):]:
        keys = ["env", "method", "nuis_acc", "nuis_mi", "inv", "xy_mse"]
        s = " | ".join([f"{k}={r.get(k)}" for k in keys])
        if "delay_ID_acc" in r:
            s += f" | delay_ID_acc={r['delay_ID_acc']}"
        if "abstraction_size" in r:
            s += f" | abstraction_size={r['abstraction_size']}"
        print(s)

    return all_results



import math, numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import seaborn as sns


ELL_LAMBDA = 1.0
ELL_BETA   = 1.0
ELL_MAX_SAMPLES_FOR_A = 200000

HEAT_NUIS_SAMPLES = 16
HEAT_ACTION_AVG   = True

BONUS_PROBE_SAMPLES = 60000
BONUS_PROBE_TEST_FRAC = 0.2
BONUS_PROBE_STEPS = 1500
BONUS_PROBE_BATCH = 512
BONUS_PROBE_EVAL = 5000

CRTR_R_LIST = [1, 2, 4, 8, 16, 32, 64]

def _onehot_a(a, n=N_ACTIONS):
    return F.one_hot(a.long(), num_classes=n).float()

@torch.no_grad()
def feat_from_enc(enc_fn, obs, a):
    z = enc_fn(obs)
    if z.ndim != 2:
        z = z.reshape(z.shape[0], -1)
    return torch.cat([z, _onehot_a(a)], dim=1)

@torch.no_grad()
def build_precision_A_from_buffer(buf, enc_fn, lam=ELL_LAMBDA, max_samples=ELL_MAX_SAMPLES_FOR_A):
    n = int(min(buf.size, max_samples))
    idx = torch.randint(0, buf.size, (n,), device=DEVICE)
    s = buf.s[idx]
    a = buf.a[idx]
    phi = feat_from_enc(enc_fn, s, a)
    d = int(phi.shape[1])
    A = lam * torch.eye(d, device=DEVICE)
    A = A + phi.T @ phi
    return A

@torch.no_grad()
def inv_from_A(A):
    return torch.linalg.inv(A)

@torch.no_grad()
def elliptical_bonus(phi, Ainv, beta=ELL_BETA, eps=1e-12):
    v = (((phi @ Ainv) * phi).sum(dim=1)).clamp_min(eps)
    return beta * torch.sqrt(v)

def plot_heatmap(heat, title, mask=None):
    h = heat.detach().cpu().numpy()
    if mask is not None:
        m = mask.detach().cpu().numpy().astype(bool)
        h[~m] = np.nan
    plt.figure(figsize=(6,6))
    sns.heatmap(h, square=True, cmap="viridis", cbar=True)
    plt.title(title)
    plt.show()

def _make_split_indices(N, test_frac=BONUS_PROBE_TEST_FRAC, seed=SEED):
    if N <= 1:
        return torch.arange(N, device=DEVICE), torch.arange(0, device=DEVICE)
    g = torch.Generator(device=DEVICE)
    g.manual_seed(int(seed) + 12345)
    perm = torch.randperm(N, generator=g, device=DEVICE)
    n_test = int(max(1, round(float(N) * float(test_frac))))
    n_test = min(n_test, N - 1)
    test_idx = perm[:n_test]
    train_idx = perm[n_test:]
    return train_idx, test_idx

def _tight_ylim(ax, vals, pad_frac=0.15):
    vmin = float(np.min(vals))
    vmax = float(np.max(vals))
    if np.isclose(vmin, vmax):
        pad = 1e-3 if vmax == 0 else abs(vmax) * 0.05
    else:
        pad = (vmax - vmin) * pad_frac
    ax.set_ylim(vmin - pad, vmax + pad)

_TETRA4 = (torch.tensor([
    [ 1.,  1.,  1.],
    [ 1., -1., -1.],
    [-1.,  1., -1.],
    [-1., -1.,  1.],
], device=DEVICE) / math.sqrt(3.0)).float()

def _obs_from_pos_rotation(pos_rc, rot):
    xy = _pos_norm_from_grid(pos_rc)
    feat = _TETRA4[rot.long()]
    return torch.cat([xy, feat], dim=1)

def _obs_from_pos_periodic(pos_rc, phase, P):
    xy = _pos_norm_from_grid(pos_rc)
    ang = (2.0 * math.pi / float(P)) * phase.float()
    ph = torch.stack([torch.sin(ang), torch.cos(ang), torch.sin(2.0 * ang)], dim=-1)
    return torch.cat([xy, ph], dim=1)

def _obs_from_pos_slippery(pos_rc, qptr):
    xy = _pos_norm_from_grid(pos_rc)
    ph = F.one_hot(qptr.long(), num_classes=3).float()
    return torch.cat([xy, ph], dim=1)

def _obs_from_pos_teacup_inside(pos_rc, phase):
    xy = _pos_norm_from_grid(pos_rc)
    feat = _TETRA4[phase.long()]
    return torch.cat([xy, feat], dim=1)

@torch.no_grad()
def _teacup_inside_mask_grid():
    centers = torch.tensor([[3,3],[3,8],[8,3],[8,8],[6,6]], device=DEVICE, dtype=torch.long)
    rad2 = 2
    grid = torch.stack(torch.meshgrid(torch.arange(MAZE_SIZE, device=DEVICE),
                                      torch.arange(MAZE_SIZE, device=DEVICE),
                                      indexing="ij"), dim=-1).reshape(-1,2)
    d2 = ((grid[:, None, :] - centers[None, :, :]).float().pow(2)).sum(dim=-1)
    inside = (d2.min(dim=1).values <= float(rad2)).reshape(MAZE_SIZE, MAZE_SIZE)
    return inside

@torch.no_grad()
def compute_heat_and_sensitivity(env_name, enc_fn, Ainv, beta=ELL_BETA):
    free = torch.nonzero(~_LAYOUT).long()
    Fcells = int(free.shape[0])
    heat_mean = torch.full((MAZE_SIZE, MAZE_SIZE), float("nan"), device=DEVICE)
    heat_std  = torch.full((MAZE_SIZE, MAZE_SIZE), float("nan"), device=DEVICE)

    actions = torch.arange(N_ACTIONS, device=DEVICE) if HEAT_ACTION_AVG else torch.zeros((1,), device=DEVICE, dtype=torch.long)

    CHUNK = 512
    for i in range(0, Fcells, CHUNK):
        pos = free[i:i+CHUNK]
        B = int(pos.shape[0])

        K = int(HEAT_NUIS_SAMPLES)

        if env_name == "Rotation":
            r = torch.randint(0, 4, (B, K), device=DEVICE)
            pos_rep = pos[:, None, :].expand(B, K, 2).reshape(-1, 2)
            r_rep = r.reshape(-1)
            obs = _obs_from_pos_rotation(pos_rep, r_rep)
            num_classes = 4

        elif env_name == "Periodicity":
            P = int(PeriodicMaze.P)
            ph = torch.randint(0, P, (B, K), device=DEVICE)
            pos_rep = pos[:, None, :].expand(B, K, 2).reshape(-1, 2)
            ph_rep = ph.reshape(-1)
            obs = _obs_from_pos_periodic(pos_rep, ph_rep, P)
            num_classes = P

        elif env_name == "Slippery":
            D = int(SlipperyDelayMaze.D)
            q = torch.randint(0, D, (B, K), device=DEVICE)
            pos_rep = pos[:, None, :].expand(B, K, 2).reshape(-1, 2)
            q_rep = q.reshape(-1)
            obs = _obs_from_pos_slippery(pos_rep, q_rep)
            num_classes = D

        elif env_name == "Teacups":
            P = int(TeacupMaze.P)
            ph = torch.randint(0, P, (B, K), device=DEVICE)
            pos_rep = pos[:, None, :].expand(B, K, 2).reshape(-1, 2)
            ph_rep = ph.reshape(-1)
            obs = _obs_from_pos_teacup_inside(pos_rep, ph_rep)
            num_classes = P

        else:
            raise ValueError(env_name)

        obs_rep = obs[:, None, :].expand(B*K, actions.numel(), OBS_DIM).reshape(-1, OBS_DIM)
        a_rep   = actions[None, :].expand(B*K, actions.numel()).reshape(-1)

        phi = feat_from_enc(enc_fn, obs_rep, a_rep)
        b = elliptical_bonus(phi, Ainv, beta=beta).reshape(B*K, actions.numel()).mean(dim=1)

        b_bk = b.reshape(B, K)
        mean = b_bk.mean(dim=1)                      
        std  = b_bk.std(dim=1, unbiased=False)       

        heat_mean[pos[:,0], pos[:,1]] = mean
        heat_std[pos[:,0], pos[:,1]]  = std

    return heat_mean, heat_std

def nuisance_predictability_from_bonus(env_name, enc_fn, Ainv, beta=ELL_BETA, n_samples=BONUS_PROBE_SAMPLES):
    free = torch.nonzero(~_LAYOUT).long()
    n = int(min(n_samples, free.shape[0] * 64))
    idx = torch.randint(0, free.shape[0], (n,), device=DEVICE)
    pos = free[idx]

    if env_name == "Rotation":
        y = torch.randint(0, 4, (n,), device=DEVICE)
        obs = _obs_from_pos_rotation(pos, y)
        C = 4
    elif env_name == "Periodicity":
        P = int(PeriodicMaze.P)
        y = torch.randint(0, P, (n,), device=DEVICE)
        obs = _obs_from_pos_periodic(pos, y, P)
        C = P
    elif env_name == "Slippery":
        D = int(SlipperyDelayMaze.D)
        y = torch.randint(0, D, (n,), device=DEVICE)
        obs = _obs_from_pos_slippery(pos, y)
        C = D
    elif env_name == "Teacups":
        P = int(TeacupMaze.P)
        y = torch.randint(0, P, (n,), device=DEVICE)
        obs = _obs_from_pos_teacup_inside(pos, y)
        C = P
    else:
        raise ValueError(env_name)

    with torch.no_grad():
        actions = torch.arange(N_ACTIONS, device=DEVICE) if HEAT_ACTION_AVG else torch.zeros((1,), device=DEVICE, dtype=torch.long)
        obs_rep = obs[:, None, :].expand(n, actions.numel(), OBS_DIM).reshape(-1, OBS_DIM)
        a_rep   = actions[None, :].expand(n, actions.numel()).reshape(-1)
        phi = feat_from_enc(enc_fn, obs_rep, a_rep)
        b = elliptical_bonus(phi, Ainv, beta=beta).reshape(n, actions.numel()).mean(dim=1)
        x = b.unsqueeze(1)

    train_idx, test_idx = _make_split_indices(n, BONUS_PROBE_TEST_FRAC, seed=SEED)

    clf = nn.Linear(1, C).to(DEVICE)
    opt = optim.Adam(clf.parameters(), lr=1e-3)

    for _ in range(BONUS_PROBE_STEPS):
        j = train_idx[torch.randint(0, train_idx.numel(), (min(BONUS_PROBE_BATCH, train_idx.numel()),), device=DEVICE)]
        logits = clf(x[j])
        loss = F.cross_entropy(logits, y[j])
        opt.zero_grad(set_to_none=True)
        loss.backward()
        opt.step()

    with torch.no_grad():
        m = min(BONUS_PROBE_EVAL, int(test_idx.numel()))
        j = test_idx[torch.randint(0, test_idx.numel(), (m,), device=DEVICE)]
        logits = clf(x[j])
        yb = y[j]
        acc = (logits.argmax(1) == yb).float().mean().item()
        ce = F.cross_entropy(logits, yb).item()
        mi = max(0.0, float(np.log(float(C)) - ce))
    return acc, mi

@torch.no_grad()
def scalar_scores_from_heat(env_name, heat_mean, heat_std, teacup_inside_mask=None):
    free_mask = (~_LAYOUT).clone()
    mean_vals = heat_mean[free_mask]
    std_vals  = heat_std[free_mask]

    W = float((std_vals.pow(2).mean()).item())

    B = float((mean_vals.var(unbiased=False)).item())

    within_std = float(std_vals.mean().item())
    within_rel = float((std_vals / (mean_vals.abs() + 1e-8)).mean().item())

    orbit_ratio = float(W / (B + 1e-8))

    out = {
        "heat_mean": float(mean_vals.mean().item()),
        "heat_std_mean": within_std,
        "heat_rel_std_mean": within_rel,
        "orbit_ratio_W_over_B": orbit_ratio,
        "W": W,
        "B": B,
    }

    if env_name == "Teacups" and teacup_inside_mask is not None:
        inside = (teacup_inside_mask & free_mask)
        outside = ((~teacup_inside_mask) & free_mask)
        if inside.any() and outside.any():
            in_mean = float(heat_mean[inside].mean().item())
            out_mean = float(heat_mean[outside].mean().item())
            out["cup_contrast_in_minus_out"] = in_mean - out_mean
            out["inside_std_mean"] = float(heat_std[inside].mean().item())
            out["inside_rel_std_mean"] = float((heat_std[inside] / (heat_mean[inside].abs() + 1e-8)).mean().item())
        else:
            out["cup_contrast_in_minus_out"] = float("nan")
            out["inside_std_mean"] = float("nan")
            out["inside_rel_std_mean"] = float("nan")

    return out

def run_elliptical_bonus_heatmaps_with_scalars():
    envs = [
        ("Rotation", RotationMaze),
        ("Periodicity", PeriodicMaze),
        ("Slippery", SlipperyDelayMaze),
        ("Teacups", TeacupMaze),
    ]

    teacup_inside_mask = _teacup_inside_mask_grid()

    all_scalar_rows = []

    for env_name, ctor in envs:
        print("\n" + "="*92)
        print(f"ELLIPTICAL BONUS + SCALARS | ENV = {env_name}")
        print("="*92)

        print("Collecting offline dataset...")
        buf, _env = collect_offline_dataset(ctor, OFFLINE_COLLECT_STEPS, OFFLINE_NUM_ENVS)
        epi = build_episode_index_strided(buf.timestep, buf.size, OFFLINE_NUM_ENVS, DEVICE)

        methods = [(f"CRTR_R{r}", ("CRTR", r)) for r in CRTR_R_LIST] +\
                  [(m, (m, None)) for m in ["IDM","ICM","RND"]] +\
                  [("BISCUIT", ("BISCUIT", None)), ("CBM", ("CBM", None))]

        for mname, (mkind, rep) in methods:
            print(f"\n>>> {mname}")

            if mkind == "CRTR":
                global CRTR_REP
                old = CRTR_REP
                CRTR_REP = int(rep)
                learner = OfflineRepLearner("CRTR").to(DEVICE)
                print("  train...", end="", flush=True)
                learner.train_steps(buf, epi, OFFLINE_TRAIN_STEPS, OFFLINE_BATCH_SIZE, log_every=PRINT_TRAIN_EVERY)
                enc_fn = lambda x, L=learner: L.rep_enc(x)
                CRTR_REP = old

            elif mkind in ["IDM","ICM","RND"]:
                learner = OfflineRepLearner(mkind).to(DEVICE)
                print("  train...", end="", flush=True)
                learner.train_steps(buf, epi, OFFLINE_TRAIN_STEPS, OFFLINE_BATCH_SIZE, log_every=PRINT_TRAIN_EVERY)
                enc_fn = lambda x, L=learner: L.rep_enc(x)

            elif mkind == "BISCUIT":
                model = BISCUIT_VAE().to(DEVICE)
                print("  train...", end="", flush=True)
                for t in range(OFFLINE_TRAIN_STEPS):
                    s, a, sp, d = buf.sample(OFFLINE_BATCH_SIZE)
                    loss = model.train_step(s, a, sp, t, OFFLINE_TRAIN_STEPS)
                    if PRINT_TRAIN_EVERY and ((t + 1) % PRINT_TRAIN_EVERY == 0):
                        print(f"    BISCUIT step {t+1:>6}/{OFFLINE_TRAIN_STEPS} | loss={float(loss.item()):.4f}", flush=True)
                enc_fn = lambda x, M=model: M.encode_mean(x)

            elif mkind == "CBM":
                print("  train...", end="", flush=True)
                dyn, rew, cmi_dyn, cmi_rew, G_dyn, PR = train_cbm_models(buf)
                keep_state = ancestors_in_dyn_graph(G_dyn, PR)
                enc_fn = lambda x, ks=keep_state: x[:, ks]

            else:
                raise ValueError(mkind)

            print("  build A...", end="", flush=True)
            A = build_precision_A_from_buffer(buf, enc_fn, lam=ELL_LAMBDA)
            Ainv = inv_from_A(A)
            print("  heat+scalars...", end="", flush=True)

            heat_mean, heat_std = compute_heat_and_sensitivity(env_name, enc_fn, Ainv, beta=ELL_BETA)

            bacc, bmi = nuisance_predictability_from_bonus(env_name, enc_fn, Ainv, beta=ELL_BETA)

            scal = scalar_scores_from_heat(
                env_name,
                heat_mean,
                heat_std,
                teacup_inside_mask=teacup_inside_mask if env_name == "Teacups" else None
            )
            scal["env"] = env_name
            scal["method"] = mname
            scal["bonus_nuis_acc"] = float(bacc)
            scal["bonus_nuis_mi"] = float(bmi)
            all_scalar_rows.append(scal)

            print("  done.")
            print(f"  Scalars:")
            print(f"    heat_mean={scal['heat_mean']:.4f}")
            print(f"    within_std_mean={scal['heat_std_mean']:.4f}   (LOW=bonus ignores nuisance)")
            print(f"    within_rel_std_mean={scal['heat_rel_std_mean']:.4f}  (LOW)")
            print(f"    orbit_ratio_W/B={scal['orbit_ratio_W_over_B']:.4f}   (LOW)")
            print(f"    bonusnuis probe: acc={bacc:.3f}, MI={bmi:.3f}  (LOW)")
            if env_name == "Teacups":
                print(f"    cup_contrast(in-out)={scal.get('cup_contrast_in_minus_out', float('nan')):.4f} (HIGH=structure sensitivity)")
                print(f"    inside_std_mean={scal.get('inside_std_mean', float('nan')):.4f} (LOW)")
                print(f"    inside_rel_std_mean={scal.get('inside_rel_std_mean', float('nan')):.4f} (LOW)")

            plot_heatmap(heat_mean, f"{env_name} | {mname} | Elliptical bonus mean  (A)")
            plot_heatmap(heat_std,  f"{env_name} | {mname} | Bonus nuisance sensitivity  Std_nuis(E_a b)")

            if env_name == "Teacups":
                plot_heatmap(heat_mean, f"{env_name} | {mname} | bonus mean (cup-only)", mask=teacup_inside_mask)
                plot_heatmap(heat_std,  f"{env_name} | {mname} | nuisance sensitivity (cup-only)", mask=teacup_inside_mask)

        env_rows = [r for r in all_scalar_rows if r["env"] == env_name]
        order = [f"CRTR_R{r}" for r in CRTR_R_LIST] + ["IDM","ICM","RND","BISCUIT","CBM"]
        env_rows = sorted(env_rows, key=lambda x: order.index(x["method"]) if x["method"] in order else 999)

        def _bar(key, title, higher_better=False):
            names = [r["method"] for r in env_rows]
            vals = np.array([r.get(key, np.nan) for r in env_rows], dtype=np.float64)
            plt.figure(figsize=(22,4))
            sns.barplot(x=names, y=vals)
            plt.title(f"{env_name}: {title}")
            ax = plt.gca()
            _tight_ylim(ax, vals)
            plt.xticks(rotation=45, ha="right")
            plt.tight_layout()
            plt.show()

        _bar("heat_std_mean", "Within-state nuisance sensitivity (Std_nuis E_a b)  [LOW=better]")
        _bar("heat_rel_std_mean", "Relative nuisance sensitivity (Std / mean)  [LOW=better]")
        _bar("orbit_ratio_W_over_B", "Orbit ratio W/(B+eps)  [LOW=better]")
        _bar("bonus_nuis_acc", "Predict nuisance from bonus (acc)  [LOW=better]")
        if env_name == "Teacups":
            _bar("cup_contrast_in_minus_out", "Cup contrast (inside - outside)  [HIGH=better]")

    return all_scalar_rows

def run_teacups_elliptical_only(
    methods=None,
    do_plots=False,                                               
    inv_samples_for_A=None,                                                                              
):
                                 
    env_name = "Teacups"
    ctor = TeacupMaze
    teacup_inside_mask = _teacup_inside_mask_grid()

    if methods is None:
        methods = [(f"CRTR_R{r}", ("CRTR", r)) for r in CRTR_R_LIST] +\
                  [(m, (m, None)) for m in ["IDM","ICM","RND"]] +\
                  [("BISCUIT", ("BISCUIT", None)), ("CBM", ("CBM", None))]

    print("\n" + "="*92)
    print(f"ELLIPTICAL BONUS + SCALARS | ENV = {env_name}  (ONLY)")
    print("="*92)

                                                
    print("Collecting offline dataset (once)...", flush=True)
    buf, _env = collect_offline_dataset(ctor, OFFLINE_COLLECT_STEPS, OFFLINE_NUM_ENVS)
    epi = build_episode_index_strided(buf.timestep, buf.size, OFFLINE_NUM_ENVS, DEVICE)

    rows = []

                                        
    for mname, (mkind, rep) in methods:
        print(f"\n>>> {mname}", flush=True)

                                     
        if mkind == "CRTR":
            global CRTR_REP
            old = CRTR_REP
            CRTR_REP = int(rep)
            learner = OfflineRepLearner("CRTR").to(DEVICE)
            learner.train_steps(buf, epi, OFFLINE_TRAIN_STEPS, OFFLINE_BATCH_SIZE, log_every=PRINT_TRAIN_EVERY)
            enc_fn = lambda x, L=learner: L.rep_enc(x)
            CRTR_REP = old

        elif mkind in ["IDM","ICM","RND"]:
            learner = OfflineRepLearner(mkind).to(DEVICE)
            learner.train_steps(buf, epi, OFFLINE_TRAIN_STEPS, OFFLINE_BATCH_SIZE, log_every=PRINT_TRAIN_EVERY)
            enc_fn = lambda x, L=learner: L.rep_enc(x)

        elif mkind == "BISCUIT":
            model = BISCUIT_VAE().to(DEVICE)
            for t in range(OFFLINE_TRAIN_STEPS):
                s, a, sp, d = buf.sample(OFFLINE_BATCH_SIZE)
                loss = model.train_step(s, a, sp, t, OFFLINE_TRAIN_STEPS)
                if PRINT_TRAIN_EVERY and ((t + 1) % PRINT_TRAIN_EVERY == 0):
                    print(f"    BISCUIT step {t+1:>6}/{OFFLINE_TRAIN_STEPS} | loss={float(loss.item()):.4f}", flush=True)
            enc_fn = lambda x, M=model: M.encode_mean(x)

        elif mkind == "CBM":
            dyn, rew, cmi_dyn, cmi_rew, G_dyn, PR = train_cbm_models(buf)
            keep_state = ancestors_in_dyn_graph(G_dyn, PR)
            enc_fn = lambda x, ks=keep_state: x[:, ks]

        else:
            raise ValueError(mkind)

                                             
        A = build_precision_A_from_buffer(buf, enc_fn, lam=ELL_LAMBDA)
        Ainv = inv_from_A(A)

        heat_mean, heat_std = compute_heat_and_sensitivity(env_name, enc_fn, Ainv, beta=ELL_BETA)

        bacc, bmi = nuisance_predictability_from_bonus(env_name, enc_fn, Ainv, beta=ELL_BETA)

        scal = scalar_scores_from_heat(
            env_name,
            heat_mean,
            heat_std,
            teacup_inside_mask=teacup_inside_mask
        )

        scal["env"] = env_name
        scal["method"] = mname
        scal["bonus_nuis_acc"] = float(bacc)
        scal["bonus_nuis_mi"] = float(bmi)
        rows.append(scal)

                                                        
        print(f"  heat_std_mean (LOW)           = {scal['heat_std_mean']:.6f}")
        print(f"  orbit_ratio W/(B+eps) (LOW)   = {scal['orbit_ratio_W_over_B']:.6f}")
        print(f"  bonusnuis acc/MI (LOW)       = {bacc:.4f} / {bmi:.4f}")
        print(f"  cup contrast in-out (HIGH)    = {scal.get('cup_contrast_in_minus_out', float('nan')):.6f}")
        print(f"  inside_std_mean (LOW)         = {scal.get('inside_std_mean', float('nan')):.6f}")
        print(f"  inside_rel_std_mean (LOW)     = {scal.get('inside_rel_std_mean', float('nan')):.6f}")

        if do_plots:
            plot_heatmap(heat_mean, f"{env_name} | {mname} | bonus mean", mask=None)
            plot_heatmap(heat_std,  f"{env_name} | {mname} | nuisance sensitivity", mask=None)
            plot_heatmap(heat_mean, f"{env_name} | {mname} | bonus mean (cup-only)", mask=teacup_inside_mask)
            plot_heatmap(heat_std,  f"{env_name} | {mname} | nuisance sensitivity (cup-only)", mask=teacup_inside_mask)

                                                         
                                        
    order = [f"CRTR_R{r}" for r in CRTR_R_LIST] + ["IDM","ICM","RND","BISCUIT","CBM"]
    rows = sorted(rows, key=lambda x: order.index(x["method"]) if x["method"] in order else 999)

    return rows


         
teacups_elliptical_rows = run_teacups_elliptical_only(do_plots=False)